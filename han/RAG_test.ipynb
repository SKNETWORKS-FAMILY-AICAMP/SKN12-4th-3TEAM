{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f36cc44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def make_chroma_db(documents):\n",
    "    # Chunking\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    docs = splitter.split_documents(documents)\n",
    "\n",
    "    # 벡터 저장소 만들기\n",
    "    db = Chroma.from_documents(docs, OpenAIEmbeddings(), persist_directory=\"chroma_db\")\n",
    "    return db\n",
    "\n",
    "def get_top5_docs_from_db(query):\n",
    "    db = Chroma(persist_directory=\"chroma_db\", embedding_function=OpenAIEmbeddings())\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 5}) # 상위 5개만 추출하도록 설정\n",
    "\n",
    "    return retriever.get_relevant_documents(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9e813c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI API 키 설정 (환경변수 또는 직접 입력)\n",
    "client = OpenAI()\n",
    "\n",
    "# 🔍 GPT를 사용해 요약 생성\n",
    "def summarize_with_gpt(content: str, file_path: str, max_chars: int = 1500) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    다음은 '{file_path}'라는 파일의 코드입니다. \n",
    "    이 파일의 목적이 무엇인지, 어떤 기능이 있고 어떤 문제를 해결하는지 간단히 요약해 주세요. \n",
    "    \\n\\n```python\\n{content[:max_chars]}\\n```\\n\\n요약:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1\",  # 또는 gpt-3.5-turbo\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ GPT 요약 실패 ({file_path}): {e}\")\n",
    "        return \"요약 실패\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eee5bed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Git] Git이 설치되어 있습니다: C:\\Program Files\\Git\\cmd\\git.EXE\n",
      "[오류] 올바른 GitHub 저장소 URL을 입력해주세요.\n",
      "예시: https://github.com/octocat/Hello-World\n",
      "또는: https://github.com/octocat/Hello-World/blob/main/README.md\n",
      "[오류] 올바른 GitHub 저장소 URL을 입력해주세요.\n",
      "예시: https://github.com/octocat/Hello-World\n",
      "또는: https://github.com/octocat/Hello-World/blob/main/README.md\n",
      "\n",
      "[정보] 저장소 소유자: AnsirH\n",
      "[정보] 저장소 이름: LANGCHAIN\n",
      "\n",
      "[정보] 전체 저장소 내용을 가져오는 중...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\USER\\Desktop\\GitHub\\3rd_project\")  # chahae 폴더의 상위 폴더\n",
    "\n",
    "from chahae.github_repo_viewer import main\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "documents = main(os.environ.get(\"GITHUB_TOKEN\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96354467",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db = make_chroma_db(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b39d088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'file_path': '03_PromptTemplate.ipynb', 'sha': '5f590d28b92557296352981f3f6250dd0259b346', 'source': 'https://github.com/AnsirH/LANGCHAIN/blob/main/03_PromptTemplate.ipynb', 'type': 'file', 'file_name': '03_PromptTemplate.ipynb', 'size': 16527}, page_content='\"# 템플릿 문자열 정의\\\\n\",\\n    \"template_str = (\\\\n\",\\n    \"    \\\\\"당신은 최고 수준의 마케팅 카피라이터입니다.\\\\\\\\n\\\\\"\\\\n\",\\n    \"    \\\\\"아래 제품의 매력적인 홍보 문구를 100자 이내로 작성해주세요.\\\\\\\\n\\\\\\\\n\\\\\"\\\\n\",\\n    \"    \\\\\"제품 명: {product_name}\\\\\\\\n\\\\\"\\\\n\",\\n    \")\\\\n\",\\n    \"\\\\n\",\\n    \"# 템플릿 객체 생성\\\\n\",\\n    \"product_prompt = PromptTemplate.from_template(template_str)\\\\n\",\\n    \"\\\\n\",\\n    \"# 프롬프트에 제품 이름을 삽입\\\\n\",\\n    \"product_name = \\\\\"스마트폰\\\\\"\\\\n\",\\n    \"formatted_prompt = product_prompt.format(product_name=product_name)\\\\n\",\\n    \"# 프롬프트 출력\\\\n\",\\n    \"print(formatted_prompt)\"'),\n",
       " Document(metadata={'file_name': '03_PromptTemplate.ipynb', 'type': 'file', 'size': 16527, 'sha': '5f590d28b92557296352981f3f6250dd0259b346', 'source': 'https://github.com/AnsirH/LANGCHAIN/blob/main/03_PromptTemplate.ipynb', 'file_path': '03_PromptTemplate.ipynb'}, page_content='\"\\\\n\",\\n    \"# 출력 파서 설정\\\\n\",\\n    \"parser = StrOutputParser()\\\\n\",\\n    \"\\\\n\",\\n    \"# 템플릿을 이용해서 문장을 완성\\\\n\",\\n    \"question = \\\\\"파이썬에서 리스트를 정렬하는 방법은 무엇인가요?\\\\\"\\\\n\",\\n    \"\\\\n\",\\n    \"# 프롬프트 | llm | parser\\\\n\",\\n    \"chain = chat_prompt | llm | parser\\\\n\",\\n    \"\\\\n\",\\n    \"response = chain.invoke({\\\\\"question\\\\\": question})\\\\n\",\\n    \"# 응답 출력\\\\n\",\\n    \"print(response)\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"id\": \"17378408\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"# PartialPromptTemplate\\\\n\",'),\n",
       " Document(metadata={'type': 'file', 'sha': '5f590d28b92557296352981f3f6250dd0259b346', 'file_path': '03_PromptTemplate.ipynb', 'source': 'https://github.com/AnsirH/LANGCHAIN/blob/main/03_PromptTemplate.ipynb', 'file_name': '03_PromptTemplate.ipynb', 'size': 16527}, page_content='\"source\": [\\n    \"# PartialPromptTemplate\\\\n\",\\n    \"- 템플릿의 일부를 부분적으로 채운 새로운 템플릿\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 20,\\n   \"id\": \"0fab1819\",\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\\\\n\",\\n    \"role_system_template = \\\\\"당신은 {rule} 분야의 전문 지식인 입니다. 가능한 자세히 답변해주세요.\\\\\"\\\\n\",\\n    \"system_prompt =SystemMessagePromptTemplate.from_template(role_system_template)\\\\n\",'),\n",
       " Document(metadata={'file_name': '01_LCEL.ipynb', 'size': 8395, 'type': 'file', 'file_path': '01_LCEL.ipynb', 'source': 'https://github.com/AnsirH/LANGCHAIN/blob/main/01_LCEL.ipynb', 'sha': '81012205169ca77d0d5e37269a0b158156498b2c'}, page_content='\"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"커피 제품을 생산하는 회사 이름은 뭘로 하면 좋을까?\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"from langchain_core.prompts import PromptTemplate\\\\n\",\\n    \"\\\\n\",\\n    \"template = \\\\\"{product} 제품을 생산하는 회사 이름은 뭘로 하면 좋을까?\\\\\"\\\\n\",\\n    \"prompt = PromptTemplate.from_template(template)\\\\n\",\\n    \"formated_prompt = prompt.format(product=\\\\\"커피\\\\\")\\\\n\",\\n    \"print(formated_prompt)\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",'),\n",
       " Document(metadata={'file_path': '03_PromptTemplate.ipynb', 'size': 16527, 'file_name': '03_PromptTemplate.ipynb', 'source': 'https://github.com/AnsirH/LANGCHAIN/blob/main/03_PromptTemplate.ipynb', 'sha': '5f590d28b92557296352981f3f6250dd0259b346', 'type': 'file'}, page_content='\"# summary_prompt = PromptTemplate.from_template(multi_template_str)\\\\n\",\\n    \"\\\\n\",\\n    \"# 포멧팅을 통해 프롬프트 값 설정\\\\n\",\\n    \"title = \\\\\"AI 기술의 발전과 미래\\\\\"\\\\n\",\\n    \"keywords = \\\\\"인공지능, 머신러닝, 딥러닝\\\\\"\\\\n\",\\n    \"formatted_summary_prompt = summary_prompt.format(title=title, keywords=keywords)\\\\n\",\\n    \"\\\\n\",\\n    \"# 프롬프트 출력\\\\n\",\\n    \"print(formatted_summary_prompt)\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 12,\\n   \"id\": \"76c98f79\",\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"prompt template 관련 코드\"\n",
    "results = get_top5_docs_from_db(query)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "403b4980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24300\\1783291465.py:6: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24300\\1783291465.py:9: LangChainDeprecationWarning: This function is deprecated. Refer to this guide on retrieval and question answering with sources: https://python.langchain.com/docs/how_to/qa_sources/\n",
      "See also the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "  qa_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_24300\\1783291465.py:10: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"input_documents\": results, \"question\": query}, return_only_outputs=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 설명 결과:\n",
      " The code related to the prompt template includes defining a template string, creating a template object, inserting a product name into the prompt, and printing the formatted prompt. It also includes setting up an output parser, using the template to complete a sentence, and printing the response. There are also examples of creating a PartialPromptTemplate, which is a new template that partially fills in part of the template. Other examples include formatting the prompt value through formatting and printing the prompt.\n",
      "SOURCES: https://github.com/AnsirH/LANGCHAIN/blob/main/03_PromptTemplate.ipynb, https://github.com/AnsirH/LANGCHAIN/blob/main/01_LCEL.ipynb\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "\n",
    "# 4. LLM 준비\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "# 5. 문서를 기반으로 설명 생성 (Chain 사용)\n",
    "qa_chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
    "result = qa_chain({\"input_documents\": results, \"question\": query}, return_only_outputs=True)\n",
    "\n",
    "# 6. 출력\n",
    "print(\"📝 설명 결과:\\n\", result[\"output_text\"])\n",
    "# print(\"\\n📚 참고된 문서 정보:\\n\", result[\"sources\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac38b91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': '하노이탑 코드 경로는 다음과 같습니다: [바로가기](https://github.com/wonwookim/coding_test_study/tree/main/week_2)\\nSOURCES: https://github.com/hwangchahae/coding_test_study/blob/main/README.md'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc581aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3rd_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
